{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------ SCRIPT TO RUN CODE ON GOOGLE COLAB FOR GPU ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import softmax\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR  # Import StepLR from torch.optim.lr_scheduler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from model import BertSentimentClassifier\n",
    "from data import SentimentDataSet\n",
    "\n",
    "def init_metrics():\n",
    "    return {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "def print_metrics(metrics, split):\n",
    "    print(f'\\n[{split}]: ', end='')\n",
    "    for k, v in metrics.items():\n",
    "        avg_value = np.mean(v) if isinstance(v, list) else v\n",
    "        print(f' {k}: {avg_value:.2f}', end='')\n",
    "\n",
    "def get_metrics(targets, predictions, average_method):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(targets, predictions),\n",
    "        'f1': f1_score(targets, predictions, average=average_method, zero_division=0),\n",
    "        'precision': precision_score(targets, predictions, average=average_method, zero_division=0),\n",
    "        'recall': recall_score(targets, predictions, average=average_method, zero_division=0)\n",
    "    }\n",
    "\n",
    "def iterate(dataloader, model, loss_fn, optimizer, train=True):\n",
    "    epoch_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Extract and check data\n",
    "        input_ids, attention_mask, et_features, targets = batch['input_ids'], batch['attention_mask'], batch['et_features'], batch['labels']\n",
    "        for tensor in [input_ids, attention_mask, et_features, targets]:\n",
    "            if tensor is not None and (torch.isnan(tensor).any() or torch.isinf(tensor).any()):\n",
    "                raise ValueError(\"NaN or Inf in tensor\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids, attention_mask, targets = input_ids.cuda(), attention_mask.cuda(), targets.cuda()\n",
    "            et_features = et_features.cuda() if et_features is not None else None\n",
    "\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            logits = model(input_ids, attention_mask, et_features)\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predictions.extend(torch.softmax(logits, dim=1).argmax(dim=1).cpu().numpy())\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss, get_metrics(all_targets, all_predictions, 'macro' if model.num_labels > 2 else 'binary')\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--num-sentiments', type=int, default=3, help='2: binary classification, 3: ternary.')\n",
    "    parser.add_argument('--use-gaze', action='store_true', help='Use gaze features if set')\n",
    "    parser.add_argument('--word-features-file', type=str, required=True, help='Path to the word level features file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dataset = SentimentDataSet('sentiment_labels_task1.csv', args.word_features_file)\n",
    "    lstm_units = 300 if args.num_sentiments == 2 else 150\n",
    "\n",
    "    XE_loss = CrossEntropyLoss()\n",
    "    train_metrics = init_metrics()\n",
    "    test_metrics = init_metrics()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for k, (train_loader, test_loader) in enumerate(dataset.split_cross_val(10)):\n",
    "        model = BertSentimentClassifier(lstm_units, args.num_sentiments, args.use_gaze)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)  # Consider reducing lr if needed\n",
    "\n",
    "        for e in range(10):\n",
    "            train_loss, train_results = iterate(train_loader, model, XE_loss, optimizer)\n",
    "            test_loss, test_results = iterate(test_loader, model, XE_loss, optimizer, train=False)\n",
    "\n",
    "            print(f'Epoch {e + 1}:')\n",
    "            print_metrics(train_results, 'TRAIN')\n",
    "            print_metrics(test_results, 'TEST')\n",
    "\n",
    "            if test_loss < best_val_loss:\n",
    "                best_val_loss = test_loss\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, 'best_model.pth')\n",
    "\n",
    "    print('\\n\\n> 10-fold CV done')\n",
    "    print_metrics(train_metrics, 'MEAN TRAIN')\n",
    "    print_metrics(test_metrics, 'MEAN TEST')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
